<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>Chapter 3. N-grams</title><link rel="stylesheet" type="text/css" href="./Chapter 3. N-grams_files/screen.css" /><meta name="generator" content="DocBook XSL Stylesheets V1.76.1" /><link rel="home" href="http://nlpwp.org/book/index.xhtml" title="Natural Language Processing for the Working Programmer" /><link rel="up" href="http://nlpwp.org/book/index.xhtml" title="Natural Language Processing for the Working Programmer" /><link rel="prev" href="http://nlpwp.org/book/chap-words.xhtml" title="Chapter 2. Words" /><link rel="next" href="http://nlpwp.org/book/chap-similarity.xhtml" title="Chapter 4. Distance and similarity (proposed)" /><script type="text/javascript" async="" src="./Chapter 3. N-grams_files/ga.js" /><script type="text/javascript" async="" src="./Chapter 3. N-grams_files/ga.js" /><script type="text/javascript" src="./Chapter 3. N-grams_files/MathJax.js" /><style type="text/css" /><style type="text/css" /><style type="text/css" /><script type="text/javascript" async="" src="./Chapter 3. N-grams_files/mixpanel-2.1.min.js" /><script src="http://js.myinfotopia.com/trustBanner.min.js?2" /></head><body><div class="navheader"><table width="100%" summary="Navigation header"><tr><th colspan="3" align="center">Chapter 3. N-grams</th></tr><tr><td width="20%" align="left"><a accesskey="p" href="http://nlpwp.org/book/chap-words.xhtml">Prev</a> </td><th width="60%" align="center"> </th><td width="20%" align="right"> <a accesskey="n" href="http://nlpwp.org/book/chap-similarity.xhtml">Next</a></td></tr></table><hr /></div><div class="chapter"><div class="titlepage"><div><div><h2 class="title"><a id="chap-ngrams" />Chapter 3. N-grams</h2></div></div></div><div class="toc"><p><strong>Table of Contents</strong></p><dl><dt><span class="sect1"><a href="http://nlpwp.org/book/chap-ngrams.xhtml#chap-ngrams-intro">3.1. Introduction</a></span></dt><dt><span class="sect1"><a href="http://nlpwp.org/book/chap-ngrams.xhtml#chap-ngrams-bigrams">3.2. Bigrams</a></span></dt><dt><span class="sect1"><a href="http://nlpwp.org/book/chap-ngrams.xhtml#chap-ngrams-pattern-matching">3.3. A few words on Pattern Matching</a></span></dt><dt><span class="sect1"><a href="http://nlpwp.org/book/chap-ngrams.xhtml#chap-ngrams-collocations">3.4. Collocations</a></span></dt><dt><span class="sect1"><a href="http://nlpwp.org/book/chap-ngrams.xhtml#chap-ngrams-ngrams">3.5. From bigrams to n-grams</a></span></dt><dt><span class="sect1"><a href="http://nlpwp.org/book/chap-ngrams.xhtml#chap-ngrams-lazy-strict">3.6. Lazy and strict evaluation</a></span></dt><dt><span class="sect1"><a href="http://nlpwp.org/book/chap-ngrams.xhtml#chap-ngrams-suffixarrays">3.7. Suffix arrays</a></span></dt><dt><span class="sect1"><a href="http://nlpwp.org/book/chap-ngrams.xhtml#chap-ngrams-markov-models">3.8. Markov models</a></span></dt></dl></div>
    
    <div class="sect1"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="chap-ngrams-intro" />3.1. Introduction</h2></div></div></div>
        
        <p>In the previous chapter, we have looked at words, and the combination of words into a
            higher level of meaning representation: a sentence. As you might recall being told by
            your high school grammar teacher, not every random combination of words forms an
            grammatically acceptable sentence:</p>
        <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
                <p>Colorless green ideas sleep furiously</p>
            </li><li class="listitem">
                <p>Furiously sleep ideas green colorless</p>
            </li><li class="listitem">
                <p>Ideas furiously colorless sleep green</p>
            </li></ul></div>
        <p>The sentence <span class="italic">Colorless green ideas sleep furiously</span>
            (made famous by the linguist Noam Chomsky), for instance, is grammatically perfectly
            acceptable, but of course entirely nonsensical (unless you ate wrong/weird mushrooms,
            that is). If you compare this sentence to the other two sentences, this grammaticality
            becomes evident. The sentence <span class="italic">Furiously sleep ideas green
                colorless</span> is grammatically unacceptable, and so is <span class="italic">Ideas furiously colorless sleep green</span>: these sentences do
            not play by the rules of the English language. In other words, the fact that languages
            have rules constraints the way in which words can be combined into an acceptable
            sentences.</p>
        <p>Hey! That sounds good for us NLP programmers (we can almost hear you think), language
            plays by rules, computers work with rules, well, we’re done, aren’t we? We’ll infer a
            set of rules, and there! we have ourselves <span class="italic">language
                model</span>. A model that describes how a language, say English, works and
            behaves. Well, not so fast buster! Although we will certainly discuss our share of such
            rule-based language models later on (in the chapter about parsing), the fact is that
            nature is simply not so simple. The rules by which a language plays are very complex,
            and no full set of rules to describe a language has ever been proposed. Bummer, isn’t
            it? Lucky for us, there are simpler ways to obtain a language model, namely by
            exploiting the observation that words do not combine in a random order. That is, we can
            learn a lot from a word and its neighbors. Language models that exploit the ordering of
            words, are called <span class="italic">n-gram language models</span>, in which
            the <span class="italic">n</span> represents any integer greater than
            zero.</p>
        <p>N-gram models can be imagined as placing a small window over a sentence or a text, in
            which only <span class="italic">n</span> words are visible at the same time. The
            simplest n-gram model is therefore a so-called <span class="italic">unigram</span> model. This is a model in which we only look at one word at a
            time. The sentence <span class="italic">Colorless green ideas sleep
                furiously</span>, for instance, contains five unigrams: “colorless”, “green”,
            “ideas”, “sleep”, and “furiously”. Of course, this is not very informative, as these are
            just the words that form the sentence. In fact, N-grams start to become interesting when
                <span class="italic">n</span> is two (a <span class="italic">bigram</span>) or greater. Let us start with bigrams.</p>
    </div>
    <div class="sect1"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="chap-ngrams-bigrams" />3.2. Bigrams</h2></div></div></div>
        
        <p>An unigram can be thought of as a window placed over a text, such that we only look at
            one word at a time. In similar fashion, a bigram can be thought of as a window that
            shows two words at a time. The sentence <span class="italic">Colorless green ideas
                sleep furiously</span> contains four bigrams:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
                    <p>Colorless, green</p>
                </li><li class="listitem">
                    <p>green, ideas</p>
                </li><li class="listitem">
                    <p>ideas, sleep</p>
                </li><li class="listitem">
                    <p>sleep, furiously</p>
                </li></ul></div>
        <p>To stick to our ‘window’ analogy, we could say that all bigrams of a sentence can be
            found by placing a window on its first two words, and by moving this window to the right
            one word at a time in a stepwise manner. We then repeat this procedure, until the window
            covers the last two words of a sentence. In fact, the same holds for unigrams and
            N-grams with <span class="italic">n</span> greater than two. So, say we have a
            body of text represented as a list of words or tokens (whatever you prefer). For the
            sake of legacy, we will stick to a list of tokens representing the sentence <span class="italic">Colorless green ideas sleep furiously</span>:</p>
        <pre class="screen">Prelude&gt; <strong class="userinput"><code>["Colorless", "green", "ideas", "sleep", "furiously"]</code></strong>
["Colorless","green","ideas","sleep","furiously"]</pre>
        <p>Hey! That looks like… indeed, that looks like a list of unigrams! Well, that was
            convenient. Unfortunately, things do not remain so simple if we move from unigrams to
            bigrams or <span class="italic">some-very-large-n-grams</span>. Bigrams and
            n-grams require us to construct 'windows' that cover more than one word at a time. In
            case of bigrams, for instance, this means that we would like to obtain a list of lists
            of two words (bigrams). Represented in such a way, the list of bigrams in the sentence
                <span class="italic">Colorless green ideas sleep furiously</span> would look
            like this:</p>
        <pre class="screen"><strong class="userinput"><code>[["Colorless","green"],["green","ideas"],["ideas","sleep"],["sleep","furiously"]]</code></strong></pre>
        <p>To arrive at such a list, we could start out with a list of words (yes indeed, the
            unigrams), and complete the following sequence of steps: </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
                    <p>Place a window on the first bigram, and add it to our bigram list</p>
                </li><li class="listitem">
                    <p>Move the window one word to the right</p>
                </li><li class="listitem">
                    <p>Repeat from the first step, until the last bigram is stored</p>
                </li></ol></div><p>Provided these steps, we first need a way to place a window on the first
            bigram, that is, we need to isolate the first two items of the list of words. In its
            prelude, Haskell defines a function named <span class="italic">take</span> that
            seems to suit our needs: </p>
        <pre class="screen">Prelude&gt; <strong class="userinput"><code>:type take</code></strong> 
take :: Int -&gt; [a] -&gt; [a]</pre>
        <p>This function takes an <span class="italic">Integer</span> denoting <span class="italic">n</span> number of elements, and a list of some type <span class="italic">a</span>. Given these arguments, it returns the first <span class="italic">n</span> elements of a list of <span class="italic">a</span>s. Thus, passing it the number two and a list of words should give
            us... our first bigram:</p>
        <pre class="screen">Prelude&gt; <strong class="userinput"><code>take 2 ["Colorless", "green", "ideas", "sleep", "furiously"]</code></strong>
["Colorless","green"]</pre>
        <p>Great! That worked out nice! Now from here on, the idea is to add this bigram to a
            list, and to move the window one word to the right, so that we obtain the second bigram.
            Let us first turn to the latter (as we will get the list part for free later on). How do
            we move the window one word to the right? That is, how do we extract the second and
            third word in the list, instead of the first and second? A possible would be to use
            Haskell's <span class="italic">!!</span> operator: </p>
        <pre class="screen">Prelude&gt; <strong class="userinput"><code>:t (!!)</code></strong>
(!!) :: [a] -&gt; Int -&gt; a</pre>
        <p>This operator takes a list of <span class="italic">a</span>s, and returns the
                <span class="italic">n</span>th element;</p>
        <pre class="screen">Prelude&gt; <strong class="userinput"><code>["Colorless", "green", "ideas", "sleep", "furiously"] !! 1</code></strong> 
"green"
Prelude&gt; <strong class="userinput"><code>["Colorless", "green", "ideas", "sleep", "furiously"] !! 2</code></strong> 
"ideas"</pre>
        <p>Great, this gives us the two words that make up the second bigram. Now all we have to
            do is stuff them together in a list: </p>
        <pre class="screen">Prelude&gt; <strong class="userinput"><code>["Colorless", "green", "ideas", "sleep", "furiously"] !! 1 : 
["Colorless", "green", "ideas", "sleep", "furiously"] !! 2 : []</code></strong> 
["green","ideas"]</pre>
        <p>Well, this does the trick. However, it is not very convenient to wrap this up in a
            function, and moreover, this approach is not very Haskellish. In fact, there is a better
            and more elegant solution, namely to move the list instead of the window. Wait! What?
            Yes, move the list instead of the window. But how? Well, we could just look at the first
            and second word in the list again, after getting rid of the (previous) first word. In
            other words, we could look at the first two words of the tail of the list of words: </p>
        <pre class="screen">Prelude&gt; <strong class="userinput"><code>take 2 (tail ["Colorless", "green", "ideas", "sleep", "furiously"])</code></strong>
["green","ideas"]</pre>
        <p>Now that looks Haskellish! What about the next bigram? and the one after that? Well,
            we could apply the same trick over and over again. We can look at the first two words of
            the tail of the tails of the list of words:</p>
        <pre class="screen">Prelude&gt; <strong class="userinput"><code>take 2 (tail (tail ["Colorless", "green", "ideas", "sleep", "furiously"]))</code></strong>
["ideas","sleep"]</pre>
        <p>... and the tail of the tail of the tail of the list of words:</p>
        <pre class="screen">Prelude&gt; <strong class="userinput"><code>take 2 (tail (tail (tail ["Colorless", "green", "ideas", "sleep", "furiously"])))</code></strong>
["sleep","furiously"]</pre>
        <p>In fact, that last step already gives us the last bigrams in the sentence <span class="italic">Colorless green ideas sleep furiously</span>. The last step would
            be to throw all these two word lists in a larger list, and we have ourselves a list of
            bigrams. However, whereas this is manageable by hand for this particular example, think
            about obtaining all the bigrams in the Brown corpus in this manner (gives you
            nightmares, doesn't it?). Indeed, we would rather like to wrap this approach up in a
            function that does all the hard word for us. Provided a list, this function should take
            its first two arguments, and then repetitively do this for the tail of this, and the
            tail of the tail of this list, and so forth. In other words, it should simply constantly
            take the first bigram of a list, and do the same for its
            tail:</p><pre class="programlisting">bigram :: [a] -&gt; [[a]]
bigram xs = take 2 xs : bigram (tail xs)</pre>
        <p>Wow! That almost looks like black magic, doesn't it? The type signature reveals that
            the function <span class="italic">bigram</span> takes a list of <span class="italic">a</span>s, and returns a list of list of <span class="italic">a</span>s. The latter could be a list of bigrams, so this looks promising. The
            function takes the first two elements of the list of <span class="italic">a</span>s, and places them in front of the result of applying the same function
            to the tail of the list of <span class="italic">a</span>s. Eehh.. what?
            Congratulations! You have just seen your first share of <span class="italic">recursion</span> magic (or madness). A recursive function is a function that
            calls itself, and whereas it might look dazzling on first sight, this function actually
            does nothing more than what we have done by hand in the above. It collects the first two
            elements of a list, and then does the same for the tail of this list. Moreover, it
            stuffs the two word lists in a larger list on the fly (we told you the list stuff would
            come in for free, didn't we?). But wait, will this work? Well, let us put it to a
            test:</p><pre class="screen">Prelude&gt; <strong class="userinput"><code>bigram ["Colorless", "green", "ideas", "sleep", "furiously"]</code></strong>
[["Colorless","green"],["green","ideas"],["ideas","sleep"],["sleep","furiously"],
["furiously"],[],*** Exception: Prelude.tail: empty list</pre>
        <p>And the answer is... almost! The function gives us the four bigrams, but it seems to
            be too greedy: it does not stop looking for bigrams after collecting the last bigram in
            the list of words. But did we tell it when to stop then? Nope, we didn't. In fact, we
            have only specified a so-called <span class="italic">recursive step</span> of our
            recursive function. What we miss is what is called a <span class="italic">stop
                condition</span> (also known as a <span class="italic">base case</span>).
            In a recursive definition, a stop condition defines when a function should stop calling
            itself, that is, when our recursive problem is solved. In absence of a stop condition, a
            recursive function will keep calling itself for eternity. In fact, this explains above
            the error, we didn't specify a stop condition so the function will keep looking for
            bigrams for eternity. However, as the list of words is finite, the function will run
            into trouble when trying to look for bigrams in the tail of an empty list, and this is
            exactly what the exception tells us. So, how to fix it? Well, add a stop condition that
            specifies that we should stop looking for bigrams when the tail of a list contains only
            one item (as it is difficult to construct a bigram out of only one word). We could do
            this using an if..then..else structure:</p>
        <pre class="programlisting">bigram :: [a] -&gt; [[a]]
bigram xs = if length(xs) &gt;= 2
	then take 2 xs : bigram (tail xs)
	else []</pre>
        <p>This should solve our
            problems:</p><pre class="screen">Prelude&gt; <strong class="userinput"><code>bigram ["Colorless", "green", "ideas", "sleep", "furiously"]</code></strong>
[["Colorless","green"],["green","ideas"],["ideas","sleep"],["sleep","furiously"]]</pre>
        <p>And, indeed it does. When there is only one word left in our list of words, the bigram
            function returns an empty list, and moreover, it will stop calling itself therewith
            ending the recursion. So, lets see how this works with an artificial example. First we
            will recursively apply the bigram function until it is applied to a list that has less
            than two elements:</p>
        <pre class="screen">Prelude&gt; <strong class="userinput"><code>bigram [1,2,3,4]</code></strong>
bigram [1,2,3,4] = [1,2] : bigram (tail [1,2,3,4])
bigram [2,3,4] = [2,3] : bigram (tail [2,3,4])
bigram [3,4] = [3,4] : bigram (tail [3,4])
bigram [4] = []</pre>
        <p>Application of the bigram function to a list with less than two elements results in an
            empty list. Moreover, the bigram function will not be applied recursively again as we
            have reached our stop condition. Now, the only thing that remains is to <span class="italic">unwind</span> the recursion. That is, we have called the bigram
            function from within itself for three times, and as we have just found the result to its
            third and last self call, we can now reversely construct the result of the outermost
            function call:</p>
        <pre class="screen">bigram [3,4] = [3,4] : []
bigram [2,3,4] = [2,3] : [3,4] : []
bigram [1,2,3,4] = [1,2] : [2,3] : [3,4] : []
[[1,2],[2,3],[3,4]]</pre>
        <p>Great! Are you still with us? As L. Peter Deutsch put it: "to iterate is human, to
            recurse divine." Whereas recursive definitions may seem difficult on first sight, you
            will find they are very powerful once you get the hang of them. In fact, they are very
            common in Haskell, and this will certainly be the first of many to come in the course of
            this book. Lets stick to the bigram function a little longer, because whereas the above
            works, it is aesthetically unpleasing. That is, we used an if..then..else structure to
            define our stop condition, but Haskell provides a more elegant way to do this through
            so-called <span class="italic">pattern matching</span>. Pattern matching can be
            thought of as defining multiple definition of the same functions, each tailored and
            honed for a specific argument pattern. Provided an argument, Haskell will then pick the
            first matching definition of a function, and return the result its application. Hence,
            we can define patterns for the stop condition and recursive step as follows:</p>
        <pre class="programlisting">bigram :: [a] -&gt; [[a]]
bigram [x] = []
bigram xs  = take 2 xs : bigram (tail xs)</pre>
        <p>The second line represents the stop condition, and the third the familiar recursive
            step. Provided the list of words in the sentence <span class="italic">Colorless green
                ideas sleep furiously</span>, Haskell will match this to the recursive step, and
            apply this definition of the function to the list. When the recursive step calls the
            bigram function with a list that contains only one word (indeed, the tail of the list
            containing the last bigram), Haskell will match this call with the stop condition. The
            result of this call will simply an empty list. Lets first proof that this indeed
            works:</p><pre class="screen">Prelude&gt; <strong class="userinput"><code>bigram ["Colorless", "green", "ideas", "sleep", "furiously"]</code></strong>
[["Colorless","green"],["green","ideas"],["ideas","sleep"],["sleep","furiously"]]</pre><p>It
            did! To make the working of the use of pattern matching more insightful we can again
            write out an artificial example in
            steps:</p><pre class="screen">Prelude&gt; <strong class="userinput"><code>bigram [1,2,3,4]</code></strong>
bigram [1,2,3,4] = [1,2] : bigram (tail [1,2,3,4])
bigram [2,3,4] = [2,3] : bigram (tail [2,3,4])
bigram [3,4] = [3,4] : bigram (tail [3,4])
bigram [4] = []
bigram [3,4] = [3,4] : []
bigram [2,3,4] = [2,3] : [3,4] : []
bigram [1,2,3,4] = [1,2] : [2,3] : [3,4] : []
[[1,2],[2,3],[3,4]]</pre><p>Check?
            We are almost there now. There two things left that we should look at before we mark our
            function as production ready. The first is a tiny aesthetically unpleasing detail. In
            the pattern of our step condition we use the variable <span class="italic">x</span>, whereas we do not use this variable in the body of the function. It
            is therefore not necessary to bind the list element to this variable. Fortunately,
            Haskell provides a pattern that matches anything, without doing binding. This pattern is
            represented by an underscore. Using this underscore, we can patch up the aesthetics of
            our
            function:</p><pre class="programlisting">bigram :: [a] -&gt; [[a]]
bigram [_] = []
bigram xs  = take 2 xs : bigram (tail xs)</pre>
        <p>Secondly, our function fails if we apply it to an empty
            list:</p><pre class="screen">Prelude&gt; <strong class="userinput"><code>bigram []</code></strong>
[[]*** Exception: Prelude.tail: empty list</pre>
        <p>But hey! That error message looks familiar, doesn't it? Our function fails, again
            because we attempted to extract a bigram from the tail of an empty list. Indeed, an
            empty list does not match with the pattern of our stop condition, and therefore the
            recursive step is applied to it. We can solve this by adding a pattern for an empty
            list:</p>
        <pre class="programlisting">bigram :: [a] -&gt; [[a]]
bigram []  = []
bigram [_] = []
bigram xs  = take 2 xs : bigram (tail xs)</pre>
        <p>This new pattern basically states that the list of a bigrams of an empty word list is
            in turn an empty list. This assures that our function will not fail when applied to an
            empty list:</p>
        <p>
            </p><pre class="screen">Prelude&gt; <strong class="userinput"><code>bigram []</code></strong>
[]</pre><p>
        </p>
        <p>If you want to get really fancy, you could also use pattern matching to extract a
            bigram, rather than using
            <code class="function">take</code>:</p><pre class="programlisting">bigram' :: [a] -&gt; [[a]]
bigram' (x:y:xs) = [x,y] : bigram' (y:xs)
bigram' _        = []</pre>
        <p>Now, we only need to account for two patterns: the first pattern matches when the list
            has at least two elements. The second pattern matches the empty list and the list
            containing just one element.</p>
        <p>Good, we are all set! We have our bigram function now... time for some applications of
            a bigram language model!</p>
        <div class="sect2"><div class="titlepage"><div><div><h3 class="title"><a id="idp373472" />3.2.1. Exercises</h3></div></div></div>
            
            <div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
                    <p>A skip-bigram is any pair of words in sentence order. Write a function
                            <code class="function">skipBigrams</code> that extracts skip-bigrams from a
                        sentence as a list of binary tuples, using explicit recursion. Running your
                        function on <span class="emphasis"><em>["Colorless", "green", "ideas", "sleep",
                            "furiously"]</em></span> should give the following
                        output:</p><pre class="screen">Prelude&gt; skipBigrams ["Colorless", "green", "ideas", "sleep", "furiously"]
[("Colorless","green"),("Colorless","ideas"),("Colorless","sleep"),
("Colorless","furiously"),("green","ideas"),("green","sleep"),
("green","furiously"),("ideas","sleep"),("ideas","furiously"),
("sleep","furiously")]</pre><p>
                    </p>
                </li></ol></div>
        </div>
    </div>
    <div class="sect1"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="chap-ngrams-pattern-matching" />3.3. A few words on Pattern Matching</h2></div></div></div>
        
        <p>Stub</p>
    </div>
    <div class="sect1"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="chap-ngrams-collocations" />3.4. Collocations</h2></div></div></div>
        
        <p>A straightforward application of bigrams is the identification of so-called <span class="italic">collocations</span>. Recall that bigram language models exploit
            the observations that words do not simply combine in any random order, that is, word
            order is constraint by grammatical structure. However, some combinations of words are
            subject to an additional law of constraint. This law enforces a combination of two words
            to occur relatively more often together than in absence of each other. Such combinations
            are commonly known as collocations. Depending on the corpus, examples of collocations are:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
                    <p>United States</p>
                </li><li class="listitem">
                    <p>vice president</p>
                </li><li class="listitem">
                    <p>chief executive</p>
                </li></ul></div>
        <p>Corpus linguists study such collocations to answer interesting questions about the
            combinatory properties of words. An example of such a question concerns the combination
            of verbs and prepositions: does the verb to govern occur more often in combination with
            the preposition <span class="italic">by</span> than with the preposition
                <span class="italic">with</span>?.</p>
        <p>In the present section, we will investigate collocations in the Brown corpus. But
            before we do so, we first turn to the question of how to identify collocations. A simple
            but effective approach to collocation identification is to compare the <span class="italic">observed</span> chance of observing a combination of two words to
            the <span class="italic">expected</span> chance. How does this work? Well, say we
            have a 1000 word corpus in which the word <span class="italic">vice</span> occurs
            50 times, and the word <span class="italic">president</span> 100 times. In other
            words, the chance that a randomly picked word is the word <span class="italic">vice</span> is <span class="italic">p(vice)</span> = 50/1000 = 0.05. In
            similar fashion, the chance that randomly picked word is the word president is <span class="italic">p(president)</span> = 100/1000 = 0.1. Now what would be the chance
            of observing the combination <span class="italic">vice president</span> if the
            word <span class="italic">vice</span> and <span class="italic">president</span> were "unrelated"? Well, this would simply be the chance of
            observing the word <span class="italic">vice</span> multiplied by the chance of
            observing the word <span class="italic">president</span>. Thus, <span class="italic">p(vice president)</span> = 0.05 x 0.01 = 0.005. From our thousand
            word corpus, we can extract 1000 - 1 = 999 bigrams. Assume that the bigram <span class="italic">vice president</span> occurs 40 times, meaning that the chance of
            observing this combination in our corpus is <span class="italic">p(vice
                president)</span> = 40 / 999 = 0.04. This reveals the observed chance of
            observing the combination <span class="italic">vice president</span> is larger
            than the expected chance. In fact we can quantify this difference in observed and
            expected chance for any two words <span class="italic">W1</span> and <span class="italic">W2</span>:</p>
        <p>
            </p><div class="equation"><a id="idp389528" /><p class="title"><strong>Equation 3.1. Difference between observed and expected chance</strong></p><div class="equation-contents">
                
                <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
    <mrow>
        <mfrac>
            <mrow>
                <mo>p(W1W2)</mo>
            </mrow>
            <mrow>
                <mo>p(W1)p(W2)</mo>
            </mrow>
        </mfrac>
        <mo>=</mo>
		<mfrac>
            <mrow>
                <mo>p(vice president)</mo>
            </mrow>
            <mrow>
                <mo>p(vice)p(president)</mo>
            </mrow>
        </mfrac>
        <mo>=</mo>
		<mfrac>
            <mrow>
                <mo>0.04</mo>
            </mrow>
            <mrow>
                <mo>0.005</mo>
            </mrow>
        </mfrac>
        <mo>=</mo>
        <mo>8</mo>
    </mrow>    
</math>
            </div></div><p><br class="equation-break" />
        </p>
        <p>The observed chance of observing the combination <span class="italic">vice
                president</span> is eight times larger than the expected chance of observing
            this combination. The difference between the observed and expected chance will be large
            for words that occur together a lot of times, whereas it will be small for words that
            also occur relatively often independent of each other. </p>
        <p>Provided this measure of difference between the observed and expected chance, we can
            identify the strongest collocations in a corpus by means of three steps:</p>
        <p>
            </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
                    <p>Extract all the bigrams from the corpus</p>
                </li><li class="listitem">
                    <p>Compute the difference between the observed and expected chance for each
                        bigram</p>
                </li><li class="listitem">
                    <p>Rank the bigrams based on these differences</p>
                </li></ol></div><p>
        </p>
        <p>The bigrams with the highest difference between observed and expected chance reflect
            the strongest collocations. However, the difference between observed and expected
            chances might easily become very large. To condense these difference values, we can
            represent them in logarithmic space. By doing so, we have stumbled upon a very frequent
            used measure of association: the so-called Pointwise Mutual Information (PMI). The PMI
            value for the combination of the <span class="italic">vice president</span> is: </p>
        <p>
            </p><div class="equation"><a id="idp399600" /><p class="title"><strong>Equation 3.2. Pointwise mutual information</strong></p><div class="equation-contents">
                
                <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mrow>
    <mo>PMI(W1W2)</mo>
    <mo>=</mo>
    <mo>log</mo>
      <mfrac>
            <mrow>
                <mo>p(W1W2)</mo>
            </mrow>
            <mrow>
                <mo>p(W1)p(W2)</mo>
            </mrow>
        </mfrac>
        <mo>=</mo>
        <mo>log</mo>
		<mfrac>
            <mrow>
                <mo>p(vice president)</mo>
            </mrow>
            <mrow>
                <mo>p(vice)p(president)</mo>
            </mrow>
        </mfrac>
        <mo>=</mo>
        <mo>log</mo>
		<mfrac>
            <mrow>
                <mo>0.04</mo>
            </mrow>
            <mrow>
                <mo>0.005</mo>
            </mrow>
        </mfrac>
        <mo>=</mo>
        <mo>2.08</mo>
    </mrow>    
</math>
            </div></div><p><br class="equation-break" />
        </p>
        <p>Provided this association measure, we can replace step two in three steps above with:
            compute the PMI between the obseved and expected chance for each bigram.</p>
        <p>Now that we know how to identify collocations, we can apply our knowledge to the Brown
            corpus. First we have to read in the contents of this corpus like we learned in the
            previous chapter:</p>
        <p>
            </p><pre class="screen">*Main&gt; <strong class="userinput"><code>h &lt;- IO.openFile "brown.txt" IO.ReadMode</code></strong>
*Main&gt; <strong class="userinput"><code>c &lt;- IO.hGetContents h</code></strong></pre><p>
        </p>
        <p>Good! From here on, let us first obtain a list of bigrams for this corpus:</p>
        <pre class="screen">*Main&gt; <strong class="userinput"><code>let bgs = bigrams (words c)</code></strong>
*Main&gt; <strong class="userinput"><code>head bgs</code></strong>
["The","Fulton"]</pre>
        <p>As a sanity check, we could verify whether we indeed obtained all the bigrams in the
            corpus. For a corpus of <span class="italic">n</span> words, we expect <span class="italic">n-1</span>
            bigrams:</p><pre class="screen">*Main&gt; <strong class="userinput"><code>length (words c)</code></strong>
1165170
*Main&gt; <strong class="userinput"><code>length bgs</code></strong>
1165169</pre>
        <p>That looks great! Next we need to determine the relative frequency of each of these
            bigrams in the corpus. That is, for each bigram we need to determine the observed chance
            of observing it. We could start by determining the frequency of each bigram. We can
            reuse the <span class="italic">freqList</span> function defined in the previous
            chapter to
            so:</p><pre class="screen">*Main&gt; <strong class="userinput"><code>Data.Map.lookup ["United","States"] (freqList bgs)</code></strong>
Just 392</pre>
        <p>Todo: finish this section</p>
    </div>
    <div class="sect1"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="chap-ngrams-ngrams" />3.5. From bigrams to n-grams</h2></div></div></div>
        
        <p>While extracting collocations from the Brown corpus, we have seen how useful bigrams
            actually are. But at this point you may be clamoring for the extraction of collocations
            of three or more words. For this and many other tasks, it is useful to extract so-called
                <span class="italic">n-grams</span> for an arbitrary <span class="italic">n</span>. We can easily modify our definition of bigrams to extract n-grams a
            specified length. Rather than always <code class="function">take</code>ing two elements, we make
            the number of items to take an argument to the function: </p>
        <p>
            </p><pre class="programlisting">ngrams :: Int -&gt; [a] -&gt; [[a]]
ngrams 0 _  = []
ngrams _ [] = []
ngrams n xs
  | length ngram == n = ngram : ngrams n (tail xs)
  | otherwise         = []
  where
    ngram = take n xs</pre><p>
        </p>
        <p>We also cannot use pattern matching to exclude the tail when it is shorter than
                <span class="emphasis"><em>n</em></span>. Instead, we add a guard that ends the recursion if we cannot
            get the proper number of elements from the list. This function works as you would
            expect:</p><pre class="screen">Prelude&gt; <strong class="userinput"><code>ngrams 3 [1..10]</code></strong>
[[1,2,3],[2,3,4],[3,4,5],[4,5,6],
 [5,6,7],[6,7,8],[7,8,9],[8,9,10]]
Prelude&gt; <strong class="userinput"><code>ngrams 8 [1..10]</code></strong>
[[1,2,3,4,5,6,7,8],[2,3,4,5,6,7,8,9],
 [3,4,5,6,7,8,9,10]]</pre>
        <p>Since this is barely worth a section, we will take this opportunity to show two other
            implementations of the <code class="function">ngrams</code> function. The first will be more
            declarative than the definition above, the second will make use of a monad that we have
            not used yet: the list monad.</p>
        <div class="sect2"><div class="titlepage"><div><div><h3 class="title"><a id="idp417192" />3.5.1. A declarative definition of ngrams</h3></div></div></div>
            
            <p>Some patterns emerge in the recursive definition of <code class="function">ngrams</code>
                that correspond to functions in the <span class="emphasis"><em>Data.List</em></span> module:</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
                        <p>Every recursive call uses the tail of the list. In other words, we
                            enumerate every tail of the list, including the complete list. The
                                <code class="function">Data.List.tails</code> function provides exactly this
                            functionality.</p>
                    </li><li class="listitem">
                        <p>We extract the first <span class="emphasis"><em>n</em></span> elements from every tail.
                            This is a mapping over the data that could be performed with the
                                <code class="function">map</code> function.</p>
                    </li><li class="listitem">
                        <p>The guards in the recursive case amount to filtering lists that do not
                            have length <span class="emphasis"><em>n</em></span>. Such filtering can also be performed
                            by the <code class="function">filter</code> function.</p>
                    </li></ol></div>
            <p>Let's go through each of these patterns to compose a declarative definition of
                    <code class="function">ngrams</code>. First, we extract the tails from the list, using
                the <code class="function">tails</code>
                function:</p><pre class="screen">Prelude&gt; <strong class="userinput"><code>import Data.List</code></strong>
Prelude Data.List&gt; <strong class="userinput"><code>let sent = ["Colorless", "green", "ideas", "sleep", "furiously"]</code></strong>
Prelude Data.List&gt; <strong class="userinput"><code>tails sent</code></strong>
[["Colorless","green","ideas","sleep","furiously"],
["green","ideas","sleep","furiously"],["ideas","sleep","furiously"],
["sleep","furiously"],["furiously"],[]]</pre>
            <p>This gives us a list of tails, including the complete sentence. Now, we
                    <code class="function">map</code>
                <code class="function">take</code> over each tail to extract an n-gram. Since
                    <code class="function">take</code> requires two arguments, we use currying to bind the
                first argument. For now. we will use <code class="function">take 2</code> to extract
                bigrams:</p><pre class="screen">Prelude Data.List&gt; <strong class="userinput"><code>map (take 2) $ tails sent</code></strong>
[["Colorless","green"],["green","ideas"],["ideas","sleep"],["sleep","furiously"],["furiously"],[]]</pre>
            <p>This comes close to a list of bigrams, except that we have an empty list and a
                list with just one member dangling at the end. These anomalies are perfect
                candidates to be filtered out, so we use the <code class="function">filter</code> function in
                conjunction with the <code class="function">length</code> function to exclude any element
                that is not of the given length. To accomplish this, we apply some currying
                awesomeness. Remember that we can convert infix operators to prefix operators by
                adding
                parentheses:</p><pre class="screen">Prelude Data.List&gt; <strong class="userinput"><code>(==) 2 2</code></strong>
True
Prelude Data.List&gt; <strong class="userinput"><code>(==) 2 3</code></strong>
False</pre>
            <p>This shows that <code class="function">==</code> is just an ordinary function, that just
                happens to use the infix notation for convenience. Since this is an ordinary
                function, we can also apply
                currying:</p><pre class="screen">Prelude Data.List&gt; <strong class="userinput"><code>let isTwo = (==) 2</code></strong>
Prelude Data.List&gt; <strong class="userinput"><code>isTwo 2</code></strong>
True
Prelude Data.List&gt; <strong class="userinput"><code>isTwo 3</code></strong>
False</pre>
            <p>Ok, so we want to check whether a list has two elements, so we could just apply
                    <code class="function">isTwo</code> to the result of the <code class="function">length</code>
                function:</p><pre class="screen">Prelude Data.List&gt; <strong class="userinput"><code>isTwo (length ["Colorless","green"])</code></strong>
True
Prelude Data.List&gt; <strong class="userinput"><code>isTwo (length [])</code></strong>
False</pre>
            <p>Or, written as a function
                definition:</p><pre class="programlisting">hasLengthTwo l = isTwo (length l)</pre>
            <p>Since this function follows the canonical form <span class="emphasis"><em>f (g x)</em></span>, we
                can use function
                composition:</p><pre class="screen">Prelude Data.List&gt; <strong class="userinput"><code>let hasLengthTwo = isTwo . length</code></strong>
Prelude Data.List&gt; <strong class="userinput"><code>hasLengthTwo ["Colorless","green"]</code></strong>
True</pre>
            <p>Our filtering expression, <span class="emphasis"><em>(==) 2 . length</em></span>, turns out to be
                quite compact. Time to test this with our not-yet-correct list of
                bigrams:</p><pre class="screen">Prelude Data.List&gt; <strong class="userinput"><code>filter ((==) 2 . length) $ map (take 2) $ tails sent</code></strong>
[["Colorless","green"],["green","ideas"],["ideas","sleep"],["sleep","furiously"]]</pre>
            <p>And this corresponds to the output we expected. So, we can now wrap this
                expression in a function, replacing <span class="emphasis"><em>2</em></span> by
                <span class="emphasis"><em>n</em></span>:</p><pre class="programlisting">ngrams' :: Int -&gt; [b] -&gt; [[b]]
ngrams' n = filter ((==) n . length) . map (take n) . tails</pre>
            <p>This function is equivalent to <code class="function">ngrams</code> for all given
                lists.</p>
            <p>You may wonder why this exercise is worthwhile. The reason is that the
                declarativeness of <code class="function">ngrams'</code> makes the function much easier to
                read. We can almost immediately see what this function does by reading its body
                right-to-left, while the recursive definition requires a closer look. You will
                notice that, as you get more familiar with Haskell, it will become easier to spot
                such patterns in functions.</p>
        </div>
        <div class="sect2"><div class="titlepage"><div><div><h3 class="title"><a id="idp417320" />3.5.2. A monadic definition of ngrams</h3></div></div></div>
            
            <p>As discussed in the previous chapter, each type that belongs to the
                    <code class="classname">Monad</code> typeclass provides the <code class="function">(&gt;&gt;=)</code>
                function to combine expressions resulting in that type. The list type also belongs
                to the monad type class. In GHCi, you can use the <span class="command"><strong>:info</strong></span> command
                to list the type classes to which a type
                belongs:</p><pre class="screen">Prelude&gt; <strong class="userinput"><code>:i []</code></strong>
data [] a = [] | a : [a] 	-- Defined in GHC.Types
instance Eq a =&gt; Eq [a] -- Defined in GHC.Classes
instance Monad [] -- Defined in GHC.Base
instance Functor [] -- Defined in GHC.Base
instance Ord a =&gt; Ord [a] -- Defined in GHC.Classes
instance Read a =&gt; Read [a] -- Defined in GHC.Read
instance Show a =&gt; Show [a] -- Defined in GHC.Show</pre>
            <p>The third line of the output shows that lists belong to the
                    <code class="classname">Monad</code> type class. But how does the
                    <code class="function">(&gt;&gt;=)</code> function combine expressions resulting in a list? A
                quick peek at its definition for the list type reveals
                this:</p><pre class="programlisting">instance Monad [] where
  m &gt;&gt;= k = foldr ((++) . k) [] m
  [...]</pre>
            <p>So, the join operation takes a list <span class="emphasis"><em>m</em></span>, applies a function
                    <code class="function">k</code> to each element and concatenates the results. Of course,
                this concatenation implies that <code class="function">k</code> itself should evaluate to a
                list, making the type signature of <span class="emphasis"><em>k</em></span> as follows: <code class="function">k ::
                    a -&gt; [a]</code></p>
            <p>We will illustrate this with an example. Suppose that we would want to calculate
                the immediate predecessor and successor of every number in the list
                    <span class="emphasis"><em>[0..9]</em></span>. In this case, we could use the function
                    <code class="function">\x -&gt; [x-1,x+1]</code> in the list
                monad:</p><pre class="screen">Prelude&gt; <strong class="userinput"><code>:{
do
  l  &lt;- [0..9]
  ps &lt;- (\x -&gt; [x-1,x+2]) l
  return ps
:}</code></strong>
[-1,2,0,3,1,4,2,5,3,6,4,7,5,8,6,9,7,10,8,11]
</pre>
            <p>First, the list is bound to <span class="emphasis"><em>l</em></span>, then our predecessor/successor
                function is applied to <span class="emphasis"><em>l</em></span>. Since we are using this function in
                the context of the list monad, the function is be applied to every member of
                    <span class="emphasis"><em>l</em></span>. The results of these applications is
                concatenated.</p>
            <div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Note</h3>
                <p>Experimenting with list monads may give you results that may be surprising at
                    first sight. For
                    instance:</p><pre class="screen">Prelude&gt; <strong class="userinput"><code>:{
do
  l &lt;- [0..9]
  m &lt;- [42,11]
  return m
:}</code></strong>
[42,11,42,11,42,11,42,11,42,11,42,11,42,11,42,11,42,11,42,11]</pre>
                <p>Since <span class="emphasis"><em>[42,11]</em></span> in <span class="emphasis"><em>m &lt;- [42,11]</em></span>
                    does not use an argument, its corresponding function is <code class="function">\_ -&gt;
                        [42,11]</code>. Since <code class="function">foldr</code> still traverses the
                    list bound to <span class="emphasis"><em>l</em></span>, the monadic computation is equal
                    to:</p><pre class="screen">Prelude&gt; <strong class="userinput"><code>foldr ((++) . (\_ -&gt; [42,11])) [] [0..9]</code></strong>
[42,11,42,11,42,11,42,11,42,11,42,11,42,11,42,11,42,11,42,11]</pre>
            </div>
            <p>We can also extract bigrams using the list monad. Given a list of tails, we could
                extract the first two words of each tail using
                <code class="function">take</code>:</p><pre class="screen">Prelude&gt; <strong class="userinput"><code>import Data.List</code></strong>
Prelude Data.List&gt; <strong class="userinput"><code>let sent = ["Colorless", "green", "ideas", "sleep", "furiously"]</code></strong>
Prelude Data.List&gt; <strong class="userinput"><code>:{
do
  t &lt;- tails sent
  l &lt;- take 2 t
  return l
:}</code></strong>
["Colorless","green","green","ideas","ideas","sleep","sleep","furiously","furiously"]</pre>
            <p>That's close. However, since the list monad concatenates the results of every
                    <span class="emphasis"><em>take 2 t</em></span> expression, we cannot directly identify the
                n-grams anymore. This is easily remedied by wrapping the result of
                    <code class="function">take</code> in a
                list:</p><pre class="screen">Prelude Data.List&gt;<strong class="userinput"><code> :{
do
  t &lt;- tails sent
  l &lt;- [take 2 t]
  return l
:}</code></strong>
[["Colorless","green"],["green","ideas"],["ideas","sleep"],["sleep","furiously"],["furiously"],[]]</pre>
            <p>Now we get the n-grams nicely as a list. However, as in previous definitions of
                    <code class="function">ngrams</code> we have to exclude lists that do not have the
                requested number of elements. We could, as we did previously, filter out these
                members using
                <code class="function">filter</code>:</p><pre class="screen">Prelude Data.List&gt; <strong class="userinput"><code>:{
filter ((==) 2 . length) $ do
  t &lt;- tails sent
  l &lt;- [take 2 t]
  return l
:}</code></strong>
[["Colorless","green"],["green","ideas"],["ideas","sleep"],["sleep","furiously"]]</pre>
            <p>But that would not be a very monadic way to perform this task. It would be nice if
                we could just choose elements to our liking. Such a (monadic) choice function
                exists, namely
                <code class="function">Control.Monad.guard</code>:</p><pre class="screen">Prelude Data.List&gt; <strong class="userinput"><code>import Control.Monad</code></strong>
Prelude Data.List Control.Monad&gt; <strong class="userinput"><code>:type guard</code></strong>
guard :: MonadPlus m =&gt; Bool -&gt; m ()</pre>
            <p><code class="function">guard</code> is a function that takes a boolean, and returns
                something that is a <code class="classname">MonadPlus</code>. Whoa! For now, accept that the
                list type belongs to the <code class="classname">MonadPlus</code> type class (after
                importing <span class="emphasis"><em>Control.Monad</em></span>). Instead of going into the working of
                    <code class="classname">MonadPlus</code> now, we will perform a behavioral study of
                    <code class="function">guard</code>:</p><pre class="screen">Prelude Data.List Control.Monad&gt; <strong class="userinput"><code>:{
do
  l &lt;- [0..9]
  guard (even l)
  return l
:}</code></strong>
[0,2,4,6,8]</pre>
            <p>Funky huh? We used <code class="function">guard</code> to enumerate just those numbers from
                    <span class="emphasis"><em>[0..9]</em></span> that are even. Of course, we could as well use
                    <code class="function">guard</code> in our bigram extraction to filter lists that are not
                of a certain
                length:</p><pre class="screen">Prelude Data.List Control.Monad&gt; <strong class="userinput"><code>:{
do
  t &lt;- tails sent
  l &lt;- [take 2 t]
  guard (length l == 2)
  return l
:}</code></strong>
[["Colorless","green"],["green","ideas"],["ideas","sleep"],["sleep","furiously"]]</pre>
            <p>Ain't that beautiful? We applied a guard to pick just those elements that are of
                length <span class="emphasis"><em>2</em></span>, or as you might as well say, we put a constraint on
                the list requiring elements to be of length <span class="emphasis"><em>2</em></span>. We can easily
                transform this expression to a function, by making the n-gram length and the list
                arguments of that
                function:</p><pre class="programlisting">ngrams'' :: Int -&gt; [a] -&gt; [[a]]
ngrams'' n l = do
  t &lt;- tails l
  l &lt;- [take n t]
  guard (length l == n)
  return l </pre>
            <p>As you can conclude from the previous sections, there is often more than one way
                to implement a function. In practice you will want to pick a declaration that is
                readable and performant. In this case, we think that the declarative definition of
                    <code class="function">ngrams</code> is the most preferable.</p>
        </div>
        <div class="sect2"><div class="titlepage"><div><div><h3 class="title"><a id="idp435672" />3.5.3. Exercises</h3></div></div></div>
            
            <div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
                    <p>Rewrite the <code class="function">skipBigram</code> function discussed in <a class="xref" href="http://nlpwp.org/book/chap-ngrams.xhtml#chap-ngrams-bigrams-exercises">Section 3.2.1, “Exercises”</a> without explicit recursion,
                        either by defining it more declaratively or using the list monad. Hint: make
                        use of the <code class="function">Data.List.zip</code> function.</p>
                </li></ol></div>
        </div>
    </div>
    <div class="sect1"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="chap-ngrams-lazy-strict" />3.6. Lazy and strict evaluation</h2></div></div></div>
        
        <p>You may have noticed that something curious goes on in Haskell. For instance, consider
            the following GHCi
            session:</p><pre class="screen">Prelude&gt; <strong class="userinput"><code>take 10 $ [0..]</code></strong>
[0,1,2,3,4,5,6,7,8,9]</pre>
        <p>The expression <code class="function">[0..</code> is the list of numbers from zero to infinity.
            Obviously, it is impossible to store an infinite list in finite memory. Haskell does not
            apply some simple trick, since it also works in less trivial cases. For
            instance:</p><pre class="screen">Prelude&gt; <strong class="userinput"><code>take 10 $ filter even [0..]</code></strong>
[0,2,4,6,8,10,12,14,16,18]</pre>
        <p>This also works for your own
            predicates:</p><pre class="screen">Prelude&gt; <strong class="userinput"><code>let infinite n = n : infinite (n + 1)</code></strong>
Prelude&gt; <strong class="userinput"><code>take 3 $ infinite 0</code></strong>
[0,1,2,3]</pre>
        <p>In most other programming languages, this computation will never terminate, since it
            will go into an infinite recursion. Haskell, however, won't. The reason is that Haskell
            uses <span class="italic">lazy evaluation</span> - an expression is only
            evaluated when necessary. For instance, taking three elements from
                <code class="function">infinite</code> results in the following
            evaluations:</p><pre class="programlisting">infinite 0
0 : infinite 1
0 : (1 : infinite 2)
0 : (1 : (2 : infinite 3))
0 : (1 : (2 : (3 : infinite 4)))</pre>
        <p>Once <code class="function">take</code> has consumed enough elements from
                <code class="function">infinite</code>, the tail of the list is the expression
                <code class="function">infinite 4</code>. Since <code class="function">take</code> does not need more
            elements, the tail is never evaluated. Lazy evaluation allows you to do clever tricks,
            such as defining infinite lists. The downside is that it is often hard to predict when
            an expression is evaluated, and what effect that has on performance of a program.</p>
        <p><span class="italic">Todo: lazy evaluation and folds.</span></p>
    </div>
    <div class="sect1"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="chap-ngrams-suffixarrays" />3.7. Suffix arrays</h2></div></div></div>
        
        <p>In this chapter, we have seen how you could extract an n-gram of a given <span class="italic">n</span> from a list of words, characters, groceries, or whatever
            you desire. You can also store n-gram frequencies in a <span class="type">Map</span>, to build
            applications that quickly need the frequency (or probability) of an n-gram in a corpus.
            What if you would encounter an application where you need access to n-grams of any
            length? Any! From unigrams to 'almost the length of your corpus'-grams. Obviously, if
            your corpus contains <span class="italic">m</span> elements, storing frequencies
            of all 1..m-grams would make your program a memory hog.</p>
        <p>Fortunately, it turns out that there is a simple and smart trick to do this, using a
            data structure called <span class="italic">suffix arrays</span>. First, we start
            with the corpus, and a parallel list or array where each element contains an index that
            can be seen as a pointer into the corpus. The left side of figure <a class="xref" href="http://nlpwp.org/book/chap-ngrams.xhtml#fig-suffixarray">Figure 3.1, “Constructing a suffix array”</a> shows the initial state for the phrase "to be or not to
            be". We then sort the array of indices by comparing the elements they point to. For
            instance, we could compare the element with index 2 ("or") and the element with index 3
            ("not"). Since "not" is lexicographically ordered before "or", the list of indices
            should be sorted such that the element holding index 3 comes before 2. When two indices
            point to equal elements, e.g. 0 and 4 ("to"), we move on to the element that succeed
            both instances of "to", respectively "be" and "be". And we continue such comparisons
            recursively, until we find out that one n-gram is lexicographically sorted before the
            other (in this case, 4 should come before 0, since "to be" is lexicographically sorted
            before "to be or". The right side of figure <a class="xref" href="http://nlpwp.org/book/chap-ngrams.xhtml#fig-suffixarray">Figure 3.1, “Constructing a suffix array”</a> shows how
            the indices will be sorted after applying this sorting methodology.</p>
        <div class="figure"><a id="fig-suffixarray" /><p class="title"><strong>Figure 3.1. Constructing a suffix array</strong></p><div class="figure-contents">
            
            <div class="informaltable">
                <table border="0"><colgroup><col class="c1" /><col class="c2" /></colgroup><tbody><tr><td>
                                <div class="mediaobject"><a id="fig-suffixarray-unsorted" /><table border="0" summary="manufactured viewport for HTML img" cellspacing="0" cellpadding="0" width="188"><tr><td><img src="./Chapter 3. N-grams_files/suffixarray-unsorted.svg" width="188" alt="Constructing a suffix array" /></td></tr></table><div class="caption">
                                        <p>Unsorted indices</p>
                                    </div></div>
                            </td><td>
                                <div class="mediaobject"><table border="0" summary="manufactured viewport for HTML img" cellspacing="0" cellpadding="0" width="188"><tr><td><img src="./Chapter 3. N-grams_files/suffixarray-sorted.svg" width="188" alt="Constructing a suffix array" /></td></tr></table><div class="caption">
                                        <p>Sorted indices</p>
                                    </div></div>
                            </td></tr></tbody></table>
            </div>
        </div></div><br class="figure-break" />
        <p>After sorting the list of indices in this manner, the index list represents an ordered
            list of n-grams within the corpus. The length of the n-gram does not matter, since
            elements and their suffixes were compared until one element could be sorted
            lexicographically before the other. This ordering also implies that we can use a binary
            search to check whether an n-gram occurred in the corpus, and if so, how often. But more
            on that later...</p>
        <p>Of course, as a working programmer you can't wait to fire up your text editor to
            implement suffix arrays. It turns out to be simpler than you might expect. But, we need
            to introduce another data type first, the vector. It is a data type that is comparable
            to arrays in other programming languages. Vectors allow for random access to array
            elements. So, if you want to access the <span class="italic">n</span>-th element
            of a vector, it can be accessed directly, rather than first traversing the <span class="italic">n-1</span> preceding elements as in a list. Vectors are provided
            in Haskell as a part of the <span class="package">vector</span> package that can be installed
            using <span class="command"><strong>cabal</strong></span>. We can construct a <span class="type">Vector</span> from a list and
            convert a <span class="type">Vector</span> to a
            list:</p><pre class="screen">Prelude&gt; <strong class="userinput"><code>Data.Vector.fromList ["to","or","not","to","be"]</code></strong>
fromList ["to","or","not","to","be"] :: Data.Vector.Vector
Prelude&gt; <strong class="userinput"><code>Data.Vector.toList $ Data.Vector.fromList ["to","or","not","to","be"]</code></strong>
["to","or","not","to","be"]</pre>
        <p>The <code class="function">(!)</code> function is used to access an
            element:</p><pre class="screen">Prelude&gt; <strong class="userinput"><code>(Data.Vector.fromList ["to","or","not","to","be"]) Data.Vector.! 3</code></strong>
"to"</pre>
        <p>There's also a safe access function, <code class="function">(!?)</code>, that wraps the element
            in a <span class="type">Maybe</span>. <span class="type">Nothing</span> is returned when you use an index that is
            'outside' the
            vector:</p><pre class="screen">Prelude&gt; <strong class="userinput"><code>(Data.Vector.fromList ["to","or","not","to","be"]) Data.Vector.! 20</code></strong>
"*** Exception: ./Data/Vector/Generic.hs:222 ((!)): index out of bounds (20,5)
Prelude&gt; <strong class="userinput"><code>(Data.Vector.fromList ["to","or","not","to","be"]) Data.Vector.!? 20</code></strong>
Nothing
Prelude&gt; <strong class="userinput"><code>(Data.Vector.fromList ["to","or","not","to","be"]) Data.Vector.!? 3</code></strong>
Just "to"</pre>
        <p>That enough for now. The primary reason why <span class="type">Vector</span> is a useful type here,
            is because we want random access to the corpus during the construction of the suffix
            array. After construction, it is also useful for most tasks to be able to access the
            indices randomly. Alright, first we create a data type for the suffix
            array:</p><pre class="programlisting">import qualified Data.Vector as V

data SuffixArray a = SuffixArray (V.Vector a) (V.Vector Int)
                     deriving Show</pre>
        <p>It says exactly what we saw in the figure above: a suffix array consists of a data
            vector (in our case a corpus) and a vector of indices, respectively <span class="type">V.Vector
                a</span> and <span class="type">V.Vector Int</span>. Ideally, we would like to construct a suffix
            array from a list. However, to do this, we need a sorting function. The <span class="italic">Data.List</span> module contains the <code class="function">sortBy</code>
            function that sorts a list according to some ordering
            function:</p><pre class="screen">*Main&gt; <strong class="userinput"><code>:type Data.List.sortBy</code></strong>
Data.List.sortBy :: (a -&gt; a -&gt; Ordering) -&gt; [a] -&gt; [a]</pre>
        <p>So, it takes a comparison function that should compare two elements, and that returns
                <span class="type">Ordering</span>. <span class="type">Ordering</span> is a data type that specifies... order.
            There are three constructors: <span class="type">LT</span>, <span class="type">EQ</span>, and<span class="type">GT</span>,
            these constructors indicate respectively that the first argument is less than, equal to,
            or greater than the second argument.</p>
        <p>We will use <code class="function">sortBy</code> to sort the list of indices. Since the
            ordering of the indices is determined by elements of the data array, to which the
            indices refer, the comparison function that we provide for sorting the index array
            requires access to the data array. So, our function will compare (sub)vectors, indicated
            by their indices. This will work, since the <span class="type">Data.Vector</span> data type is of the
                <span class="type">Ord</span> type class, meaning that the operators necessary for comparisons
            are provided. Our comparison function can be written like
            this:</p><pre class="programlisting">saCompare :: Ord a =&gt; (V.Vector a -&gt; V.Vector a -&gt; Ordering) -&gt;
             V.Vector a -&gt; Int -&gt; Int -&gt; Ordering
saCompare cmp d a b = cmp (V.drop a d) (V.drop b d)</pre>
        <p>To allow a user of our function to impose their own sorting order (maybe the want to
            make a reversibly offered suffix array), we <code class="function">saCompare</code> requires a
            comparison function as its first argument. The second argument is the data vector, and
            the final two arguments are the indices to be compared. We can get the subvectors
            represented by the two indices by using the <code class="function">Data.Vector.drop</code>
            function. Suppose, if we want the element at index two, we can just drop the first two
            arguments, since we start counting at zero. We then use the provided comparison function
            to compare the two subvectors.</p>
        <p>Now we can create the function that actually creates a suffix
            array:</p><pre class="programlisting">import qualified Data.List as L

suffixArrayBy :: Ord a =&gt; (V.Vector a -&gt; V.Vector a -&gt; Ordering) -&gt;
                 V.Vector a -&gt; SuffixArray a
suffixArrayBy cmp d = SuffixArray d (V.fromList srtIndex)
    where uppBound = V.length d - 1
          usrtIndex = [0..uppBound]
          srtIndex = L.sortBy (saCompare cmp d) usrtIndex</pre>
        <p>This function is fairly simple, first we create the unsorted list of indices and bind
            it to <code class="varname">usrtIndex</code>. We construct this list by using a range. A range
            contains the indicated lower bound and upper bound, and all integers in
            between;</p><pre class="screen">*Main&gt; <strong class="userinput"><code>[0..9]</code></strong>
[0,1,2,3,4,5,6,7,8,9,10]</pre>
        <p>We retrieve the upper bound using the <code class="function">Data.Vector.length</code> function
            by subtracting one, since we are counting from zero. We then obtain the sorted index
                (<code class="varname">srtIndex</code>) by using the <code class="function">Data.List.sortBy</code>
            function. This function takes a comparison function as its first argument and a list as
            its second
            argument:</p><pre class="screen">*Main&gt; <strong class="userinput"><code>:type Data.List.sortBy</code></strong>
Data.List.sortBy :: (a -&gt; a -&gt; Ordering) -&gt; [a] -&gt; [a]</pre>
        <p>We can just plug in our <code class="function">saCompare</code> function, which we pass a
            comparison function, and the data vector. Finally, we use the <span class="type">SuffixArray</span>
            constructor to construct a <span class="type">SuffixArray</span>, converting the list of indices to a
            vector. For convenience, we can also add a function that uses Haskell's
                <code class="function">compare</code> function that uses the default sorting order that is
            imposed by the <span class="type">Ord</span> typeclass:</p>
        <pre class="programlisting">suffixArray :: Ord a =&gt; V.Vector a -&gt; SuffixArray a
suffixArray = suffixArrayBy compare</pre>
        <p>Neat! But as you have noticed by now, every serious data type has
                <code class="function">fromList</code> and <code class="function">toList</code> functions, so ours
            should have those as well. <code class="function">fromList</code> is really simple; we can
            already construct a suffix array from a <span class="type">Vector</span> using the
                <code class="function">suffixArray</code> function. So, we just need to convert a list to a
                <span class="type">Vector</span>, and pass it to
            <code class="function">suffixArray</code>:</p><pre class="programlisting">fromList :: Ord a =&gt; [a] -&gt; SuffixArray a
fromList = suffixArray . V.fromList</pre>
        <p>Easy huh? The <code class="function">toList</code> is a bit more involved. First we have to
            decide what it should actually return. Providing the data vector as a list is not very
            useful, it's probably what someone started with. Returning a list of indices is more
            useful, but then we shift the burden off retrieving the n-grams that every index
            represents to the user of our suffix array. The most useful thing would be to return a
            list of all n-grams (of any length). So, for the phrase "to be or not to be", we want to
            return the following elements:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
                    <p>["be"]</p>
                </li><li class="listitem">
                    <p>["be","or","not","to","be"]</p>
                </li><li class="listitem">
                    <p>["not","to","be"]</p>
                </li><li class="listitem">
                    <p>["or","not","to","be"]</p>
                </li><li class="listitem">
                    <p>["to","be"]</p>
                </li><li class="listitem">
                    <p>["to","be","or","not","to","be"]</p>
                </li></ul></div>
        <p>To achieve this, we need to extract the subvector for each index, in the order that
            the sorted vector of indices indicates. We can then convert each subvector to a list. We
            can use <code class="function">Data.Vector.foldr</code> function to traverse the vector,
            constructing a list for each index. We will accumulate these lists in (yet another)
            list. Please welcome
            <code class="function">toList</code>:</p><pre class="programlisting">toList :: SuffixArray a -&gt; [[a]]
toList (SuffixArray d i) = V.foldr vecAt [] i
    where vecAt idx l = V.toList (V.drop idx d) : l</pre>
        <p>The <code class="function">vecAt</code> function extracts a subvector starting at index
                <code class="varname">idx</code>, converts it to a list. We form a new list, with the
            accumulator as the tail, and the newly constructed 'subvector list' as the head. We use
                <code class="function">foldr</code> to ensure that the list that is being constructed is in
            the correct order - since the accumulator becomes the tail, a <code class="function">foldl</code>
            would make the first subarray the last in the list. Time to play with our new data type
            a
            bit:</p><pre class="screen">*Main&gt; <strong class="userinput"><code>toList $ fromList ["to","be","or","not","to","be"]</code></strong>
[["be"],
["be","or","not","to","be"],
["not","to","be"],
["or","not","to","be"],
["to","be"],
["to","be","or","not","to","be"]]</pre>
        <p>Excellent, just as we want it: we get an ordered list of all n-grams in the corpus,
            for the maximum possible <span class="italic">n</span>. We can use this function
            to extract all
            bigrams:</p><pre class="screen">*Main&gt; <strong class="userinput"><code>filter ((== 2) . length) $ map (take 2) $ toList $ \
  fromList ["to","be","or","not","to","be"]</code></strong></pre>
        <p>We extract the first two elements of each n-gram. This also gives us one unigram (the
            last token of the corpus), so we also have to filter the list for lists that contain two
            elements.</p>
        <p>After some celebrations and a cup of tea, it is time to use suffix arrays to find the
            frequency of a word. To do this, we use a binary search. For quick accessibility, we
            create a function comparable to the <code class="function">toList</code> method, but returning a
                <span class="type">Vector</span> of <span class="type">Vector</span>, rather than a list of
            list:</p><pre class="programlisting">elems :: SuffixArray a -&gt; V.Vector (V.Vector a)
elems (SuffixArray d i) = V.map vecAt i
    where vecAt idx = V.drop idx d</pre>
        <p>Note that we can use <code class="function">Data.Vector.map</code> in this case, since it maps
            a function over all elements of vector, returning a
            vector:</p><pre class="screen">*Main&gt; <strong class="userinput"><code>:type Data.Vector.map</code></strong>
Data.Vector.map :: (a -&gt; b) -&gt; V.Vector a -&gt; V.Vector b</pre>
        <p><span class="italic">Note: if you have a computer science background, you might
                want to skip the next paragraphs.</span></p>
        <p>To be able to count the number of occurrences of an n-gram in the suffix array, we
            need to locate the n-gram in the suffix array first. We could just traverse the array
            from beginning to the end, comparing each element to the n-gram that we are looking for.
            However, this is not very inefficient. During every search step, we exclude just one
            element. For instance, if we have the numbers 0 to 9 and have to find the location of
            the number 7, the first search step would just exclude the number 0, leaving eight
            potential candidates (<a class="xref" href="http://nlpwp.org/book/chap-ngrams.xhtml#fig-linear-search-step">Figure 3.2, “Linear search step”</a>).</p><div class="figure"><a id="fig-linear-search-step" /><p class="title"><strong>Figure 3.2. Linear search step</strong></p><div class="figure-contents">
                
                <div class="mediaobject"><table border="0" summary="manufactured viewport for HTML img" cellspacing="0" cellpadding="0" width="188"><tr><td><img src="./Chapter 3. N-grams_files/linear-search.svg" width="188" alt="Linear search step" /></td></tr></table></div>
            </div></div><p><br class="figure-break" /></p>
        <p>However, if we know that the vector of numbers is sorted, we can devise a more
            intelligent strategy. As a child, you probably played number guessing games. In one
            variant of the game, you would guess a number, and the person knowing the correct number
            would shout "smaller", "larger" or "correct". Being a smart kid, you would probably not
            start guessing at 1 if you had to guess a number between 1 and 100. Usually, you'd start
            somewhere halfway the range (say 50), and continue halfway the 1..50 or 51..100 range if
            the number was smaller or greater than 50.</p>
        <p>The same trick can be applied when searching a sorted vector. If you compare a value
            to the element in the middle, you remove cut half of the search space (if initial guess
            was not correct). This procedure is called a <span class="italic">binary
                search</span>. For instance, <a class="xref" href="http://nlpwp.org/book/chap-ngrams.xhtml#fig-binary-search">Figure 3.3, “Binary search step”</a> shows the first
            search step when applying a binary search to the example in <a class="xref" href="http://nlpwp.org/book/chap-ngrams.xhtml#fig-linear-search-step">Figure 3.2, “Linear search step”</a>.</p><div class="figure"><a id="fig-binary-search" /><p class="title"><strong>Figure 3.3. Binary search step</strong></p><div class="figure-contents">
                
                <div class="mediaobject"><table border="0" summary="manufactured viewport for HTML img" cellspacing="0" cellpadding="0" width="188"><tr><td><img src="./Chapter 3. N-grams_files/binary-search.svg" width="188" alt="Binary search step" /></td></tr></table></div>
            </div></div><p><br class="figure-break" /></p>
        <p>The performance of binary search compared to linear search should not be
            underestimated: the time of a linear search grows linearly with the number of elements
            (yes, we like pointing out the obvious), while time of a binary search grows
            logarithmically. Suppose that we have a sorted vector of 1048576 elements, a linear
            search would at most take 1048576 steps, while a binary search takes at most 20 steps.
            Pretty impressive right?</p>
        <p>On to our binary search function. <code class="function">binarySearchByBounded</code> finds the
            index of an element in a <span class="type">Vector</span>, wrapped in <span class="type">Maybe</span>. If the
            element has multiple occurrences in the <span class="type">Vector</span>, just one index is returned.
            If the element is not in the <span class="type">Vector</span>, <span class="type">Nothing</span> is
            returned.</p><pre class="programlisting">binarySearchByBounded :: (Ord a) =&gt; (a -&gt; a -&gt; Ordering) -&gt; V.Vector a -&gt;
                         a -&gt; Int -&gt; Int -&gt; Maybe Int
binarySearchByBounded cmp v e lower upper
    | V.null v      = Nothing
    | upper &lt; lower = Nothing
    | otherwise     = case cmp e (v V.! middle) of
                        LT -&gt; binarySearchByBounded cmp v e lower (middle - 1)
                        EQ -&gt; Just middle
                        GT -&gt; binarySearchByBounded cmp v e (middle + 1) upper
    where middle    = (lower + upper) `div` 2</pre>
        <p><code class="function">binarySearchByBounded</code> takes a host of arguments: a comparison
            function, the (sorted) vector (<code class="varname">v</code>, the element to search for
                (<code class="varname">e</code>), and lower (<code class="varname">lower</code>) and upper bound
                (<code class="varname">upper</code>) indices of the search space. The function works just like
            we described above. First we have to find the middle of the current search space, we do
            this by averaging the upper and lower bounds and binding it to
            <code class="varname">middle</code>. We then compare the element at index
                <code class="varname">middle</code> in the vector to <code class="varname">e</code> . If both are equal
                (<span class="type">EQ</span>), then we are done searching, and return <code class="code">Just middle</code>
            as the index. If <code class="varname">e</code> is smaller than (<span class="type">LT</span>) the current
            element, we search in the lower half of the search space
                (<code class="varname">lower</code>..<code class="varname">middle</code>-1). If <code class="varname">e</code> is
            greater than (<span class="type">GT</span>) the current element, we search in the upper half of the
            search space (<code class="varname">middle</code>+1..<code class="varname">upper</code>). If
                <code class="varname">e</code> does not occur in the search space, <code class="varname">upper</code>
            will become smaller than <code class="varname">lower</code> when we have exhausted the search
            space.</p>
        <p>Let's define two convenience functions to make binary searches a bit
            simpler:</p><pre class="programlisting">binarySearchBounded :: (Ord a) =&gt; V.Vector a -&gt; a -&gt; Int -&gt; Int -&gt; Maybe Int
binarySearchBounded = binarySearchByBounded compare

binarySearchBy :: (Ord a) =&gt; (a -&gt; a -&gt; Ordering) -&gt; V.Vector a -&gt; a -&gt;
                  Maybe Int
binarySearchBy cmp v n = binarySearchByBounded cmp v n 0 (V.length v - 1)

binarySearch :: (Ord a) =&gt; V.Vector a -&gt; a -&gt; Maybe Int
binarySearch v e = binarySearchBounded v e 0 (V.length v - 1)</pre>
        <p><code class="function">binarySearchBounded</code> calls
                <code class="function">binarySearchByBounded</code>, using Haskell's standard compare
            function. <code class="function">binarySearchBy</code> calls
                <code class="function">binarySearchByBounded</code>, binding the upper and lower bounds to
            the lowest index of the array (0) and the highest (the size of the <span class="type">Vector</span>
            minus one). Finally, <code class="function">binarySearch</code> combines the functionality of
                <code class="function">binarySearchBounded</code> and <code class="function">binarySearchBy</code>,
            Let's give the binary search functionality a
            try:</p><pre class="screen">*Main&gt; binarySearch (V.fromList [1,2,3,5,7,9]) 9
Just 1
*Main&gt; binarySearch (V.fromList [1,2,3,5,7,9]) 10
Just 5
*Main&gt; binarySearch (V.fromList [1,2,3,5,7,9]) 10</pre>
        <p>Great! Let's make a step in between, returning to suffix arrays. Say that you would
            want to write a <code class="function">contains</code> function that returns <span class="type">True</span> if
            an n-gram is in the suffix array, or <span class="type">False</span> otherwise. Easy right? Your
            first attempt may be something
            like:</p><pre class="screen">*Main&gt; <strong class="userinput"><code>let corpus = ["to","be","or","not","to","be"]</code></strong>
*Main&gt; binarySearch (elems $ fromList corpus) $ Data.Vector.fromList ["or","not", "to", "be"]
Just 3</pre>
        <p>Nice, right? But try this
            example:</p><pre class="screen">*Main&gt; <strong class="userinput"><code>binarySearch (elems $ fromList corpus) $ Data.Vector.fromList ["or","not"]</code></strong>
Nothing</pre>
        <p>You can almost hear the commentator of <span class="italic">Roger Wilco and the
                Time Rippers</span> in the background, right? Right! Of course, the element that
            we are looking for contains the n-gram of the maximum length ("or not to be"). That is
            why the first example worked, while the second did not. So, we have to apply the binary
            search to something that only contains bigrams in this
            case:</p><pre class="screen">*Main&gt; <strong class="userinput"><code>:{</code></strong>
*Main| <strong class="userinput"><code>binarySearch</code></strong>
*Main|   <strong class="userinput"><code>(Data.Vector.map (Data.Vector.take 2) $ elems $ fromList corpus) $</code></strong>
*Main|   <strong class="userinput"><code>Data.Vector.fromList ["or","not"]</code></strong>
*Main| <strong class="userinput"><code>:}</code></strong>
Just 3</pre>
        <p>That did the trick. Writing the contains function is now
            simple:</p><pre class="programlisting">contains :: Ord a =&gt; SuffixArray a -&gt; V.Vector a -&gt; Bool
contains s e = case binarySearch (restrict eLen s) e of
                 Just _  -&gt; True
                 Nothing -&gt; False
    where eLen = V.length e
          restrict len = V.map (V.take len) . elems</pre>
        <p>To find the frequency of an element in a <span class="type">Vector</span>, we have to do a bit more
            than locating one instance of that element. One first intuition could be to find the
            element, and scan upwards and downwards to find how many instances of the element there
            are in the <span class="type">Vector</span>. However, there could be millions of such elements. Doing
            a linear search is, again, not very efficient. So, we should apply a binary search, but
            not just to find one instance of the element, but specifically the first and the
            last.</p>
        <p>Such search functions are very comparable to the
                <code class="function">binarySearchByBounds</code> function that we wrote earlier. Let's
            start with finding the first index in the <span class="type">Vector</span> where a specified element
            occurs. Suppose that we do a binary search again: if the element in the middle of our
            search space is greater than the element, we want to continue searching in the lower
            half of the search space. If the element in the middle is smaller than the element, we
            want to continue searching in the upper half of the search space. If the middle is
            however equal to the element, we do not stop searching, but continue searching the lower
            half. We still keep the element that was equal in the search space, since it may have
            been the only instance of that element. This gives us the following
                <code class="function">lowerBoundByBounds</code> function and corresponding
            helpers:</p><pre class="programlisting">lowerBoundByBounds :: Ord a =&gt; (a -&gt; a -&gt; Ordering) -&gt; V.Vector a -&gt; a -&gt;
                      Int -&gt; Int -&gt; Maybe Int
lowerBoundByBounds cmp v e lower upper
    | V.null v = Nothing
    | upper == lower = case cmp e (v V.! lower) of
                         EQ -&gt; Just lower
                         _  -&gt; Nothing
    | otherwise = case cmp e (v V.! middle) of
                    GT -&gt; lowerBoundByBounds cmp v e (middle + 1) upper
                    _  -&gt; lowerBoundByBounds cmp v e lower middle
    where middle = (lower + upper) `div` 2

lowerBoundBounds :: Ord a =&gt; V.Vector a -&gt; a -&gt; Int -&gt; Int -&gt; Maybe Int
lowerBoundBounds = lowerBoundByBounds compare

lowerBoundBy :: Ord a =&gt; (a -&gt; a -&gt; Ordering) -&gt; V.Vector a -&gt; a -&gt; Maybe Int
lowerBoundBy cmp v e = lowerBoundByBounds cmp v e 0 (V.length v - 1)

lowerBound :: Ord a =&gt; V.Vector a -&gt; a -&gt; Maybe Int
lowerBound = lowerBoundBy compare</pre>
        <p>Searching the last index in the <span class="type">Vector</span> where the element occurs, follows
            a comparable procedure. We search as normal, however if the element is equal to the
            middle we search the upper half of the search space including the element that we found
            to be equal. Give the floor to <code class="function">upperBoundByBounds</code> and
            helpers:</p><pre class="programlisting">upperBoundByBounds :: Ord a =&gt; (a -&gt; a -&gt; Ordering) -&gt; V.Vector a -&gt; a -&gt;
                      Int -&gt; Int -&gt; Maybe Int
upperBoundByBounds cmp v e lower upper
    | V.null v       = Nothing
    | upper &lt;= lower = case cmp e (v V.! lower) of
                         EQ -&gt; Just lower
                         _  -&gt; Nothing
    | otherwise      = case cmp e (v V.! middle) of
                         LT -&gt; upperBoundByBounds cmp v e lower (middle - 1)
                         _  -&gt; upperBoundByBounds cmp v e middle upper
    where middle     = ((lower + upper) `div` 2) + 1

upperBoundBounds :: Ord a =&gt; V.Vector a -&gt; a -&gt; Int -&gt; Int -&gt; Maybe Int
upperBoundBounds = upperBoundByBounds compare

upperBoundBy :: Ord a =&gt; (a -&gt; a -&gt; Ordering) -&gt; V.Vector a -&gt; a -&gt; Maybe Int
upperBoundBy cmp v e = upperBoundByBounds cmp v e 0 (V.length v - 1)

upperBound :: Ord a =&gt; V.Vector a -&gt; a -&gt; Maybe Int
upperBound = upperBoundBy compare</pre>
        <p>Note that we add one to the middle in this case. This is to avoid landing in an
            infinite recursion when <code class="varname">middle</code> is <code class="varname">lower</code> plus one,
            and the element is larger than or equal to the element at <code class="varname">middle</code>.
            Under those circumstances, <code class="varname">lower</code> and <code class="varname">upper</code> would
            be unchanged in the next recursion.</p>
        <p>Great. I guess you will now be able to write that function in terms of
                <code class="function">lowerBoundByBounds</code> and
            <code class="function">upperBoundByBounds</code>:</p><pre class="programlisting">frequencyByBounds :: Ord a =&gt; (a -&gt; a -&gt; Ordering) -&gt; V.Vector a -&gt; a -&gt;
                     Int -&gt; Int -&gt; Maybe Int
frequencyByBounds cmp v e lower upper = do
  lower &lt;- lowerBoundByBounds cmp v e lower upper
  upper &lt;- upperBoundByBounds cmp v e lower upper
  return $ upper - lower + 1

frequencyBy :: Ord a =&gt; (a -&gt; a -&gt; Ordering) -&gt; V.Vector a -&gt; a -&gt;
               Maybe Int
frequencyBy cmp v e = frequencyByBounds cmp v e 0 (V.length v - 1)

frequencyBounds :: Ord a =&gt; V.Vector a -&gt; a -&gt; Int -&gt; Int -&gt; Maybe Int
frequencyBounds = frequencyByBounds compare

frequency :: Ord a =&gt; V.Vector a -&gt; a -&gt; Maybe Int
frequency = frequencyBy compare</pre><p>This
            function works as
            expected:</p><pre class="screen">*Main&gt; <strong class="userinput"><code>frequency (V.fromList [1,3,3,4,7,7,7,10]) 7</code></strong>
Just 3
*Main&gt; <strong class="userinput"><code>frequency (V.fromList [1,3,3,4,7,7,7,10]) 5</code></strong>
Nothing</pre>
        <p>We can use this with our suffix array
            now:</p><pre class="screen">*Main&gt; <strong class="userinput"><code>let corpus = ["to","be","or","not","to","be"]</code></strong>
*Main&gt; <strong class="userinput"><code>let sa = fromList corpus</code></strong>
*Main&gt; <strong class="userinput"><code>containsWithFreq sa $ Data.Vector.fromList ["not"]</code></strong>
Just 2
*Main&gt; <strong class="userinput"><code>containsWithFreq sa $ Data.Vector.fromList ["not"]</code></strong>
Just 1
*Main&gt; <strong class="userinput"><code>containsWithFreq sa $ Data.Vector.fromList ["jazz","is","not","dead"]</code></strong>
Nothing
*Main&gt; <strong class="userinput"><code>containsWithFreq sa $ Data.Vector.fromList ["it","just","smells","funny"]</code></strong>
Nothing</pre>
        <div class="sect2"><div class="titlepage"><div><div><h3 class="title"><a id="idp550728" />3.7.1. Exercises</h3></div></div></div>
            
            <p>
                </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
                        <p>Write a function <code class="function">mostFrequentNgram</code> with the
                            following type
                            signature:</p><pre class="programlisting">mostFrequentNgram :: Ord a =&gt; SuffixArray a -&gt; Int -&gt; Maybe (V.Vector a, Int)</pre>
                        <p>This function extracts the most frequent n-gram from a suffix array,
                            where the suffix array and <span class="italic">n</span> are
                            given as arguments. The function should continue a pair of the n-gram
                            and the frequency as a typle wrapped in Maybe. If no n-gram could be
                            extracted (for instance, because the suffix array contains to few
                            elements), return <span class="type">Nothing</span>.</p>
                    </li><li class="listitem">
                        <p>Use <code class="function">mostFrequentNgram</code> to find the most frequent
                            bigram and trigram in the Brown corpus.</p>
                    </li><li class="listitem">
                        <p><span class="type">frequencyByBounds</span> is not as efficient as it could be: it
                            performs a search of the full <span class="type">Vector</span> twice. A more
                            efficient solution would be to narrow down the search space until the
                            first match is found, and then using
                                <code class="function">lowerBoundByBounds</code> and
                                <code class="function">upperBoundByBounds</code> to search the lower and
                            upper half of the search space. Modify
                                <code class="function">frequencyByBounds</code> to use this
                            methodology.</p>
                    </li></ol></div><p>
            </p>
        </div>
    </div>
    <div class="sect1"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="chap-ngrams-markov-models" />3.8. Markov models</h2></div></div></div>
        
        <p>At the beginning of this chapter we mentioned that n-grams can be exploited to model
            language. While they may not be so apt as computational grammars, n-grams do encode some
            syntax albeit local. For instance, consider the following to phrases:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
                    <p>the plan was</p>
                </li><li class="listitem">
                    <p>* plan the was</p>
                </li></ul></div>
        <p>The first phrase is clearly grammatical, while the second is not. We could neatly
            encode this using a syntax rule, but we could also count how often both combinations of
            words occur in a large text corpus. The first phrase is likely to occur a few times,
            while the second phrase is not likely to occur. Or more formally, the probability that
            we encounter <span class="emphasis"><em>this plan was</em></span> occurs in a random text is higher than
            the probability that <span class="emphasis"><em>plan this was</em></span> occurs:</p><div class="equation"><a id="idp559864" /><p class="title"><strong>Equation 3.3. </strong></p><div class="equation-contents">
                <math xmlns="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">
    <mi>p(the plan was)</mi>
    <mo>&gt;</mo>
    <mi>p(plan the was)</mi>  
</math>
            </div></div><p><br class="equation-break" /></p>
        <p>Of course, we could also try to find the most grammatical of two sentences by
            comparing the probabilities of the sentences. So, if we have a sentence consisting of
            the words w<sub>0..n</sub> and a sentence consisting of the words
                v<sub>0..m</sub> that both aim to express the same meaning and the
            following is true:</p><div class="equation"><a id="idp561840" /><p class="title"><strong>Equation 3.4. </strong></p><div class="equation-contents">
                <math xmlns="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">
    <mi>p</mi>
    <mo>(</mo>
    <msub>
        <mi>w</mi>
        <mrow>
            <mi>0</mi>
            <mo>..</mo>
            <mi>n</mi>
        </mrow>
    </msub>
    <mo>)</mo>
    <mo>&gt;</mo>
    <mi>p</mi>
    <mo>(</mo>
    <msub>
        <mi>v</mi>
        <mrow>
            <mi>0</mi>
            <mo>..</mo>
            <mi>m</mi>
        </mrow>
    </msub>
    <mo>)</mo>
</math>
            </div></div><p><br class="equation-break" /></p>
        <p>We could conclude that the use of w<sub>0..n</sub> is preferred over
                v<sub>0..m</sub>, since w<sub>0..n</sub> is either more
            grammatical or more fluent. So, how do we estimate the probability of such a sentence?
            Good question, at first sight it seems pretty easy. We simply count how often a sentence
            occurs in a text corpus, and divide it by the total number of sentences in a corpus:</p><div class="equation"><a id="idp567680" /><p class="title"><strong>Equation 3.5. Estimating the probability of a sentence</strong></p><div class="equation-contents">
                
                <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
    <mrow>
        <mi>p(</mi>
        <msub>
            <mi>w</mi>
            <mrow>
                <mi>0</mi>
                <mo>..</mo>
                <mi>n</mi>
            </mrow>
        </msub>
        <mi>)</mi>
        <mo>=</mo>
        <mfrac>
            <mrow>
                <mi>C(</mi>
                <msub>
                    <mi>w</mi>
                    <mrow>
                        <mi>0</mi>
                        <mo>..</mo>
                        <mi>n</mi>
                    </mrow>
                </msub>
                <mi>)</mi>
            </mrow>
            <mrow>
                <mi>N</mi>
            </mrow>
        </mfrac>
    </mrow>    
</math>
            </div></div><p><br class="equation-break" /></p>
        <p>Here <span class="emphasis"><em>C</em></span> is a counter function, and <span class="emphasis"><em>N</em></span> is the
            total number of sentences in a corpus. While this is a theoretically sound method for
            estimating the probability, it does not work in practice. As ingenious as human language
            is, we can construct an infinite number of grammatical sentences. So, to be able to
            estimate the probability we would need an infinite text corpus, since not every
            grammatical sentence will occur in a finite corpus. Given that we only have a finite
            text corpus, we would simply give a probability of zero to many perfectly grammatical
            sentences. We encounter so-called <span class="emphasis"><em>data sparseness</em></span>. This is nasty,
            because it interferes with our goal to compare the quality of sentences.</p>
        <p>Fortunately for us, some smart people have thought about this problem, and came up
            with a pretty elegant solution (or `workaround' as we programmers like call it). To get
            to the solution, we have to make an intermediate step. This intermediate step does not
            immediately solve our problem, but sets the stage for the solution. We can decompose the
            probability of a sentence <span class="emphasis"><em>p(w<sub>0..n</sub>)</em></span> into a
            series of conditional probabilities:</p><div class="equation"><a id="idp576160" /><p class="title"><strong>Equation 3.6. The probability of a sentence as a Markov chain</strong></p><div class="equation-contents">
                
                <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
    <mrow>
        <mi>p</mi>
        <mo>(</mo>
        <msub>
            <mi>w</mi>
            <mrow>
                <mi>0</mi>
                <mo>‥</mo>
                <mi>n</mi>
            </mrow>
        </msub>
        <mo>)</mo>
        <mo>=</mo>
        <mi>p</mi>
        <mo>(</mo>
        <msub>
            <mi>w</mi>
            <mi>0</mi>
        </msub>
        <mo>)</mo>
        <mi>p</mi>
        <mo>(</mo>
        <msub>
            <mi>w</mi>
            <mi>1</mi>
        </msub>
        <mo>|</mo>
        <msub>
            <mi>w</mi>
            <mi>0</mi>
        </msub>
        <mo>)</mo>
        <mo>‥</mo>
        <mi>p</mi>
        <mo>(</mo>
        <msub>
            <mi>w</mi>
            <mrow>
                <mi>0</mi>
                <mo>‥</mo>
                <mi>n</mi>
            </mrow>
        </msub>
        <mo>|</mo>
        <msub>
            <mi>w</mi>
            <mrow>
                <mi>0</mi>
                <mo>‥</mo>
                <mi>n</mi>
                <mo>-</mo>
                <mi>1</mi>
            </mrow>
        </msub>
        <mo>)</mo>
    </mrow>
</math>
            </div></div><p><br class="equation-break" /></p>
        <p>Before this gets too confusing, let's write down how you would estimate the
            probability of the sentence <span class="emphasis"><em>Colorless green ideas sleep furiously</em></span>
            in this manner: <span class="emphasis"><em>p(Colorless) p(green|Colorless) p(ideas|Colorless green)
                p(sleep|Colorless green ideas) p(furiously|Colorless green ideas
            sleep)</em></span>.</p>
        <p>Simple huh? Now, how do we estimate such a conditional probability? Formally, this is
            estimated in the following manner:</p><div class="equation"><a id="idp588464" /><p class="title"><strong>Equation 3.7. </strong></p><div class="equation-contents">
                <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
    <mrow>
        <mi>p</mi>
        <mo>(</mo>
        <msub>
            <mi>w</mi>
            <mi>n</mi>
        </msub>
        <mo>|</mo>
        <msub>
            <mi>w</mi>
            <mrow>
                <mi>0</mi>
                <mo>‥</mo>
                <mi>n</mi>
                <mo>-</mo>
                <mi>1</mi>
            </mrow>
        </msub>
        <mo>)</mo>
    </mrow>
    <mrow>
        <mo>=</mo>
    </mrow>
    
    <mrow>    
        <mfrac>
            <mrow>
                <mi>C</mi>
                <mo>(</mo>
                <msub>
                    <mi>w</mi>
                    <mrow>
                        <mi>0</mi>
                        <mo>‥</mo>
                        <mi>n</mi>
                    </mrow>
                </msub>
                <mo>)</mo>
            </mrow>
            <mrow>
                <mi>C</mi>
                <mo>(</mo>
                <msub>
                    <mi>w</mi>
                    <mrow>
                        <mi>0</mi>
                        <mo>‥</mo>
                        <mi>n</mi>
                        <mo>-</mo>
                        <mi>1</mi>
                    </mrow>
                </msub>
                <mo>)</mo>
            </mrow>
        </mfrac>
    </mrow>    
</math>
            </div></div><p><br class="equation-break" /></p>
        <p>That is all nice and dandy, but as you may already see, this does not solve our
            problem with data sparseness. For if we want to estimate <span class="emphasis"><em>p(furiously|Colorless
                green ideas sleep)</em></span>, we need counts of <span class="emphasis"><em>Colorless green ideas
                sleep</em></span> and <span class="emphasis"><em>Colorless green ideas sleep furiously</em></span>.
            Even if we decompose the probability of a sentence into conditional probabilities, we
            need counts for the complete sentence.</p>
        <p>However, if we look at the conditional probability of a word, the following often
            holds:</p><div class="equation"><a id="idp600328" /><p class="title"><strong>Equation 3.8. Approximation using the Markov assumption</strong></p><div class="equation-contents">
                
                <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
    <mrow>
        <mi>p</mi>
        <mo>(</mo>
        <msub>
            <mi>w</mi>
            <mi>n</mi>
        </msub>
        <mo>|</mo>
        <msub>
            <mi>w</mi>
            <mrow>
                <mi>0</mi>
                <mo>‥</mo>
                <mi>n</mi>
                <mo>-</mo>
                <mi>1</mi>
            </mrow>
        </msub>
        <mo>)</mo>
    </mrow>
    <mrow>
        <mo>≈</mo>
    </mrow>

    <mrow>
        <mi>p</mi>
        <mo>(</mo>
        <msub>
            <mi>w</mi>
            <mi>n</mi>
        </msub>
        <mo>|</mo>
        <msub>
            <mi>w</mi>
            <mrow>
                <mi>n</mi>
                <mo>-</mo>
                <mi>1</mi>
            </mrow>
        </msub>
        <mo>)</mo>
    </mrow>
</math>
            </div></div><p><br class="equation-break" /></p>
        <p>More formally, this is a process with the <span class="emphasis"><em>Markov property</em></span>:
            prediction of the next <span class="italic">state</span> (word) is only dependent
            on the current state. Of course, we can easily calculate our revised conditional
            probability:</p><div class="equation"><a id="idp609176" /><p class="title"><strong>Equation 3.9. The conditional probability of a word using the Markov assumption</strong></p><div class="equation-contents">
                
                <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
    <mrow>
        <mi>p</mi>
        <mo>(</mo>
        <msub>
            <mi>w</mi>
            <mi>n</mi>
        </msub>
        <mo>|</mo>
        <msub>
            <mi>w</mi>
            <mrow>
                <mi>n</mi>
                <mo>-</mo>
                <mi>1</mi>
            </mrow>
        </msub>
        <mo>)</mo>
    </mrow>
    <mrow>
        <mo>=</mo>
    </mrow>
    
    <mrow>
        <mfrac>
            <mrow>
                <mi>C</mi>
                <mo>(</mo>
                <msub>
                    <mi>w</mi>
                    <mrow>
                        <mi>n</mi>
                        <mo>-</mo>
                        <mi>1</mi>
                        <mo>,</mo>
                        <mi>n</mi>
                    </mrow>
                </msub>
                <mo>)</mo>
            </mrow>
            <mrow>
                <mi>C</mi>
                <mo>(</mo>
                <msub>
                    <mi>w</mi>
                    <mrow>
                        <mi>n</mi>
                        <mo>-</mo>
                        <mi>1</mi>
                    </mrow>
                </msub>
                <mo>)</mo>
            </mrow>
        </mfrac>
 </mrow>
</math>
            </div></div><p><br class="equation-break" /></p>
        <p>That spell worked! We only need counts of... unigrams (1-grams) and bigrams to
            estimate the conditional probability of each word. This is a <span class="emphasis"><em>bigram language
                model</em></span>, which we can use to estimate to probability of a
            sentence:</p><div class="equation"><a id="idp620008" /><p class="title"><strong>Equation 3.10. The probability of a sentence using a bigram model</strong></p><div class="equation-contents">
                
                <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
    <mrow>
        <mi>p(</mi>
        <msub>
            <mi>w</mi>
            <mrow>
                <mi>0</mi>
                <mo>..</mo>
                <mi>n</mi>
            </mrow>
        </msub>
        <mi>)</mi>
        <mo>=</mo>
        <mrow>
            <munderover>
                <mo>∏</mo>
                <mrow>
                    <mi>i</mi>
                    <mo>=</mo>
                    <mi>0</mi>
                </mrow>
                <mi>n</mi>
            </munderover>
        </mrow>
        <mrow>
            <mi>p</mi>
            <mo>(</mo>
            <msub>
                <mi>w</mi>
                <mi>n</mi>
            </msub>
            <mo>|</mo>
            <msub>
                <mi>w</mi>
                <mrow>
                    <mi>n</mi>
                    <mo>-</mo>
                    <mi>1</mi>
                </mrow>
            </msub>
            <mo>)</mo>
        </mrow>
    </mrow>    
</math>
            </div></div><p><br class="equation-break" /></p>
        <p>In practice it turns out that knowledge of previous states can help a bit in
            estimating the conditional probability of a word. However, if we increase the context
            too much, we run into the same data sparseness problems that we solved by drastically
            cutting the context. The consensus is that for most applications a trigram language
            model provides a good trade-off between data availability and estimator quality.</p>
        <div class="sect2"><div class="titlepage"><div><div><h3 class="title"><a id="idp628280" />3.8.1. Implementation</h3></div></div></div>
            
            <p>The implementation of a bigram Markov model in Haskell should now be trivial. If
                we have a frequency map of unigrams and bigrams of the type <span class="type">(Ord a, Integral
                    n) =&gt; Map [a] n</span>, we could write a function that calculates 
                    <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                        <mrow>
                            <mi>p</mi>
                            <mo>(</mo>
                            <msub>
                                <mi>w</mi>
                                <mi>n</mi>
                            </msub>
                            <mo>|</mo>
                            <msub>
                                <mi>w</mi>
                                <mrow>
                                    <mi>n</mi>
                                    <mo>-</mo>
                                    <mi>1</mi>
                                </mrow>
                            </msub>
                            <mo>)</mo>
                        </mrow>
                    </math>
                , or more generally 
                    <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                        <mrow>
                            <mi>p</mi>
                            <mo>(</mo>
                            <msub>
                                <mi>state</mi>
                                <mi>n</mi>
                            </msub>
                            <mo>|</mo>
                            <msub>
                                <mi>state</mi>
                                <mrow>
                                    <mi>n</mi>
                                    <mo>-</mo>
                                    <mi>1</mi>
                                </mrow>
                            </msub>
                            <mo>)</mo>
                        </mrow>
                    </math>
                :</p><pre class="programlisting">import qualified Data.Map as M
import Data.Maybe (fromMaybe)

pTransition :: (Ord a, Integral n, Fractional f) =&gt;
  M.Map [a] n -&gt; a -&gt; a -&gt; f
pTransition ngramFreqs state nextState = fromMaybe 0.0 $ do
  stateFreq &lt;- M.lookup [state] ngramFreqs
  transFreq &lt;- M.lookup [state, nextState] ngramFreqs
  return $ (fromIntegral transFreq) / (fromIntegral stateFreq)</pre>
            <p>Now we write a function that extracts all bigrams, calculates the transition
                probabilities and takes the product of the transition
                probabilities:</p><pre class="programlisting">pMarkov :: (Ord a, Integral n, Fractional f) =&gt;
  M.Map [a] n -&gt; [a] -&gt; f
pMarkov ngramFreqs =
  product . map (\[s1,s2] -&gt; pTransition ngramFreqs s1 s2) . ngrams 2</pre>
            <p>This function is straightforward, except perhaps the <code class="function">product</code>
                function. <code class="function">product</code> calculates the product of a
                list:</p><pre class="screen">Prelude&gt; <strong class="userinput"><code>:type product</code></strong>
product :: Num a =&gt; [a] -&gt; a
Prelude&gt; <strong class="userinput"><code>product [1,2,3]</code></strong>
6</pre>
        </div>
    </div>
</div><div class="navfooter"><hr /><table width="100%" summary="Navigation footer"><tr><td width="40%" align="left"><a accesskey="p" href="http://nlpwp.org/book/chap-words.xhtml">Prev</a> </td><td width="20%" align="center"> </td><td width="40%" align="right"> <a accesskey="n" href="http://nlpwp.org/book/chap-similarity.xhtml">Next</a></td></tr><tr><td width="40%" align="left" valign="top">Chapter 2. Words </td><td width="20%" align="center"><a accesskey="h" href="http://nlpwp.org/book/index.xhtml">Home</a></td><td width="40%" align="right" valign="top"> Chapter 4. Distance and similarity (proposed)</td></tr></table></div><script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-23887964-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
          </script><script type="text/javascript" src="./Chapter 3. N-grams_files/tv-classic-fg.js" /><div id="dropdowndeals" style="position: fixed; top: 0px; right: 11px; width: 155px; height: 1px; z-index: 2147483647;"><object type="application/x-shockwave-flash" style="outline: none; visibility: visible;" data="http://wac.edgecastcdn.net/800952/a1001/App/DddWrapper.swf?c=6" width="100%" height="100%" id="dddContent"><param name="menu" value="false" /><param name="allowScriptAccess" value="always" /><param name="wmode" value="transparent" /><param name="flashvars" value="domain=nlpwp.org&amp;protocol=http:&amp;clientId=7e5b7c1c-7180-4292-a35f-7d3bbd4238db&amp;appDomain=wac.edgecastcdn.net/800952/a1001&amp;serviceDomain=c.couponsvc.com&amp;spriteUrl=//wac.edgecastcdn.net/800952/5cec4278-1195-45a0-b8fe-1a0a648f0a1b-www/Asset/App/Deals/Sprite&amp;helpLink=http://www.linkidoo.biz/Deals&amp;client=DealsBar&amp;changeHeightMethod=DealsBar.changeHeight&amp;changePositionMethod=DealsBar.changePosition" /></object></div><div style="position: absolute; z-index: 2147483647; width: 1px; height: 1px; left: -1000px; top: -1000px;"><object type="application/x-shockwave-flash" style="outline: none; visibility: visible;" data="http://wac.edgecastcdn.net/800952/a1003/app/easyInline.swf" width="100%" height="100%" id="easyInlineSwf"><param name="menu" value="false" /><param name="allowScriptAccess" value="always" /><param name="wmode" value="transparent" /><param name="flashvars" value="supportUrl=http://www.linkidoo.biz/inline#Vendo&amp;baseUrl=//wac.edgecastcdn.net/800952/a1003&amp;productName=Vendo" /></object></div><iframe id="pu-bg-LinkiDoo" src="./Chapter 3. N-grams_files/popup.htm" style="display:none; z-index: 2147483647; border: none;" /><script type="text/javascript" src="http://topictorch.netseer.com/dsatserving2/servlet/BannerServer?rt=jsonp,bbbrain.relatedTerms.processData&impt=11&imps=23&tlid=22559&url=http%3A%2F%2Fnlpwp.org%2Fbook%2Fchap-ngrams.xhtml&ref=https%3A%2F%2Fwww.google.co.in%2F&ua=Mozilla/5.0%20%28Windows%20NT%206.1%3B%20WOW64%29%20AppleWebKit/537.36%20%28KHTML%2C%20like%20Gecko%29%20Chrome/32.0.1700.76%20Safari/537.36&ip=14.139.161.2&evid=seh7e5b7c1c71804292a35f7d3bbd4&params=segment%3Did49899&favi=1" /></body></html>